{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/cseadmin/dz/TrafficFlowModel/model'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from lib.metrics import evaluate\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "DATA_PATH=\"../data/sz_taxi_202006/\"\n",
    "vec_size=512\n",
    "\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1751602"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[198, Timestamp('2020-06-01 00:43:29'), 56.0],\n",
       " [199, Timestamp('2020-06-01 00:43:49'), 55.5],\n",
       " [448, Timestamp('2020-06-01 00:44:09'), 55.0]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj_list=np.load(\"../data/sz_taxi_202006/sz_taxi_202006_traj_list.npy\", allow_pickle=True)\n",
    "\n",
    "len(traj_list)\n",
    "traj_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1751602"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[198, 199, 448]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tracks=[]\n",
    "for traj in traj_list:\n",
    "    road_ids=[]\n",
    "    for point in traj:\n",
    "        road_ids.append(point[0])\n",
    "    all_tracks.append(road_ids)\n",
    "\n",
    "len(all_tracks)\n",
    "all_tracks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_all_traj=Word2Vec(all_tracks, sg=1, hs=1, vector_size=vec_size, window=10, min_count=1, workers=4)\n",
    "\n",
    "w2v_all_traj.save(os.path.join(DATA_PATH, f\"w2v_all_traj_{vec_size}.model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7991076111793518"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_all_traj=Word2Vec.load(os.path.join(DATA_PATH, f\"w2v_all_traj_{vec_size}.model\"))\n",
    "wv=w2v_all_traj.wv\n",
    "\n",
    "wv.distance(0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04961416,  0.03711881,  0.00701622, -0.02471929,  0.04195437,\n",
       "        0.05551275, -0.0024164 ,  0.03509161, -0.01095492,  0.05997162],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index=np.argsort(wv.index_to_key)\n",
    "embed_vectors=wv.get_normed_vectors()\n",
    "\n",
    "embed_vectors=embed_vectors[index]\n",
    "embed_vectors[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(492, 492)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1.0000002 , 0.13911077, 0.18122032, 0.20089237, 0.2514562 ,\n",
       "       0.16970405, 0.21216552, 0.12217825, 0.12732378, 0.13965628],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_matrix = embed_vectors @ embed_vectors.T\n",
    "\n",
    "correlation_matrix.shape\n",
    "correlation_matrix[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    "import math\n",
    "\n",
    "class GraphConvolution(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \n",
    "    out = A*X*W\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        print(\"inp-\", input.shape)\n",
    "        support = torch.matmul(input, self.weight) # (bs, N, hidden_dim)\n",
    "        print(\"sup-\", support.shape)\n",
    "        support=support.permute(1, 2, 0)\n",
    "        print(\"sup-\", support.shape)\n",
    "        output = torch.matmul(adj, support).permute(2, 0, 1)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "class DontKnowWhat2EatNN(torch.nn.Module):\n",
    "    def __init__(self, graph_embed_vectors, in_step, out_step, num_heads=8, drop_rate=0):\n",
    "        super(DontKnowWhat2EatNN, self).__init__()\n",
    "        \n",
    "        self.graph_embed_vectors=torch.FloatTensor(graph_embed_vectors.reshape(graph_embed_vectors.shape[0], 1, graph_embed_vectors.shape[1]))\n",
    "        self.embed_dim=graph_embed_vectors.shape[-1]\n",
    "        self.num_heads=num_heads\n",
    "        self.drop_rate=drop_rate\n",
    "        \n",
    "        self.mh_attention=torch.nn.MultiheadAttention(self.embed_dim, self.num_heads, dropout=self.drop_rate)\n",
    "        self.gcn1=GraphConvolution(in_step, 64)\n",
    "        self.bn1=torch.nn.BatchNorm2d(64)\n",
    "        self.gcn2=GraphConvolution(64, 64)\n",
    "        self.bn2=torch.nn.BatchNorm2d(64)\n",
    "        self.fc=torch.nn.Linear(64, out_step)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, N, in_step, num_features)\n",
    "        \"\"\"\n",
    "        \n",
    "        attn_output, attn_output_weights=self.mh_attention.forward(self.graph_embed_vectors, self.graph_embed_vectors, self.graph_embed_vectors)\n",
    "        attn_output=attn_output.reshape(attn_output.shape[0], attn_output.shape[2])\n",
    "        correlation_matrix=attn_output @ attn_output.T\n",
    "        \n",
    "        print(\"x-\", x.shape)\n",
    "        out=self.gcn1(x, correlation_matrix)\n",
    "        out=self.bn1(out)\n",
    "        out=self.gcn2(out, correlation_matrix)\n",
    "        out=self.bn2(out)\n",
    "        # out=out.reshape(out.shape[0], out.shape[1], -1)\n",
    "        out=self.fc(out)\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  2.],\n",
       "         [ 3.,  4.]],\n",
       "\n",
       "        [[ 5.,  6.],\n",
       "         [ 7.,  8.]],\n",
       "\n",
       "        [[ 9., 10.],\n",
       "         [11., 12.]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000],\n",
       "        [0.5000, 0.5000]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.Tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]])\n",
    "b=torch.Tensor([[0.5, 0.5], [0.5, 0.5]])\n",
    "\n",
    "a\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5000,  1.5000],\n",
       "         [ 3.5000,  3.5000]],\n",
       "\n",
       "        [[ 5.5000,  5.5000],\n",
       "         [ 7.5000,  7.5000]],\n",
       "\n",
       "        [[ 9.5000,  9.5000],\n",
       "         [11.5000, 11.5000]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  5.,  9.],\n",
       "         [ 2.,  6., 10.]],\n",
       "\n",
       "        [[ 3.,  7., 11.],\n",
       "         [ 4.,  8., 12.]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5000,  1.5000],\n",
       "         [ 3.5000,  3.5000]],\n",
       "\n",
       "        [[ 5.5000,  5.5000],\n",
       "         [ 7.5000,  7.5000]],\n",
       "\n",
       "        [[ 9.5000,  9.5000],\n",
       "         [11.5000, 11.5000]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(a, b)\n",
    "torch.matmul(b, a.permute(1, 2, 0)).permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_xy(data, in_steps, out_steps):\n",
    "    \"\"\"\n",
    "    Parameter\n",
    "    ---\n",
    "    data: (N, timesteps)\n",
    "    \n",
    "    Returns\n",
    "    ---\n",
    "    x: (num_samples, N, in_steps, num_features=1) Tensor\n",
    "    y: (num_samples, N, out_steps, num_features=1) Tensor\n",
    "        num_samples is determined by `timesteps` and `in_steps+out_steps`\n",
    "    \"\"\"\n",
    "    # Generate the beginning index and the ending index of a sample, which\n",
    "    # contains (num_points_for_training + num_points_for_predicting) points\n",
    "    all_steps=data.shape[1]\n",
    "    indices = [(i, i + (in_steps + out_steps)) for i in range(all_steps - (in_steps + out_steps) + 1)]\n",
    "    \n",
    "    x, y=[], []\n",
    "    for begin, end in indices:\n",
    "        x.append(data[:, begin: begin+in_steps])\n",
    "        y.append(data[:, begin+in_steps: end])\n",
    "        \n",
    "    x=np.array(x)\n",
    "    y=np.array(y)\n",
    "    \n",
    "    # if x.ndim==3 and y.ndim==3:\n",
    "    #     x=x[:, :, :, np.newaxis]\n",
    "    #     y=y[:, :, :, np.newaxis]\n",
    "        \n",
    "    return torch.Tensor(x), torch.Tensor(y)\n",
    "\n",
    "def read_data(data_path, file_type=\"pickle\", transpose=False):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    ---\n",
    "    X: (N, all_timesteps) numpy\n",
    "    \"\"\"\n",
    "    if file_type==\"pickle\":\n",
    "        data=pd.read_pickle(data_path)\n",
    "    elif file_type==\"csv\":\n",
    "        data=pd.read_csv(data_path)\n",
    "    else:\n",
    "        print(\"Invalid file type.\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    data=data.values.astype(np.float)\n",
    "    if transpose:\n",
    "        data=data.T\n",
    "    print(\"Original data shape\", data.shape)\n",
    "    return data\n",
    "\n",
    "def get_dataloaders(data, in_steps, out_steps, train_size=0.7, val_size=0.1, batch_size=32):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ---\n",
    "    data: (N, all_timesteps) numpy\n",
    "    \"\"\"\n",
    "    all_steps=data.shape[1]\n",
    "    split1=int(all_steps*train_size)\n",
    "    split2=int(all_steps*(train_size+val_size))\n",
    "    \n",
    "    train_data=data[:, :split1]\n",
    "    val_data=data[:, split1:split2]\n",
    "    test_data=data[:, split2:]\n",
    "    \n",
    "    x_train, y_train=gen_xy(train_data, in_steps, out_steps)\n",
    "    x_val, y_val=gen_xy(val_data, in_steps, out_steps)\n",
    "    x_test, y_test=gen_xy(test_data, in_steps, out_steps)\n",
    "    \n",
    "    print(f\"Trainset:\\tx-{x_train.size()}\\ty-{y_train.size()}\")\n",
    "    print(f\"Valset:  \\tx-{x_val.size()}  \\ty-{y_val.size()}\")\n",
    "    print(f\"Testset:\\tx-{x_test.size()}\\ty-{y_test.size()}\")\n",
    "    \n",
    "    trainset=torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    valset=torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    testset=torch.utils.data.TensorDataset(x_test, y_test)\n",
    "    \n",
    "    trainset_loader=torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    valset_loader=torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True)\n",
    "    testset_loader=torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return trainset_loader, valset_loader, testset_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, trainset_loader, optimizer, criterion, gpu=True):\n",
    "    model.train()\n",
    "    batch_loss_list=[]\n",
    "    for x_batch, y_batch in trainset_loader:\n",
    "        if gpu and torch.cuda.is_available():\n",
    "            x_batch = x_batch.cuda()\n",
    "            y_batch = y_batch.cuda()\n",
    "\n",
    "        out_batch = model.forward(x_batch)\n",
    "        loss = criterion.forward(out_batch, y_batch)\n",
    "        batch_loss_list.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return sum(batch_loss_list)/len(batch_loss_list)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, valset_loader, criterion, gpu=True):\n",
    "    model.eval()\n",
    "    batch_loss_list=[]\n",
    "    for x_batch, y_batch in valset_loader:\n",
    "        if gpu and torch.cuda.is_available():\n",
    "            x_batch = x_batch.cuda()\n",
    "            y_batch = y_batch.cuda()\n",
    "\n",
    "        out_batch = model.forward(x_batch)\n",
    "        loss = criterion.forward(out_batch, y_batch)\n",
    "        batch_loss_list.append(loss.item())\n",
    "\n",
    "    return sum(batch_loss_list)/len(batch_loss_list)\n",
    "\n",
    "def train(model, trainset_loader, valset_loader, optimizer, criterion, max_epochs=100, early_stop=10, verbose=1, gpu=True, plot=False):\n",
    "    wait=0\n",
    "    min_val_loss=np.inf\n",
    "    \n",
    "    train_loss_list=[]\n",
    "    val_loss_list=[]\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        train_loss=train_one_epoch(model, trainset_loader, optimizer, criterion, gpu)\n",
    "        train_loss_list.append(train_loss)\n",
    "        \n",
    "        val_loss=eval_model(model, valset_loader, criterion)\n",
    "        val_loss_list.append(val_loss)\n",
    "        \n",
    "        if (epoch+1)%verbose==0:\n",
    "            print(datetime.datetime.now(), \"Epoch\", epoch,\n",
    "                  \"\\tTrain Loss = %.5f\"%train_loss,\n",
    "                  \"\\tVal Loss = %.5f\"%val_loss)\n",
    "        \n",
    "        if val_loss<min_val_loss:\n",
    "            wait=0\n",
    "            min_val_loss=val_loss\n",
    "            best_epoch=epoch\n",
    "        else:\n",
    "            wait+=1\n",
    "            if wait >= early_stop:\n",
    "                print(\"Early stopping at epoch: %d\" % epoch)\n",
    "                break\n",
    "        \n",
    "    if plot:\n",
    "        plt.plot(range(0, epoch+1), train_loss_list, \"-\", label=\"Train Loss\")\n",
    "        plt.plot(range(0, epoch+1), val_loss_list, \"-\", label=\"Val Loss\")\n",
    "        plt.title(\"Epoch-Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape (492, 8064)\n",
      "Trainset:\tx-torch.Size([5621, 492, 12])\ty-torch.Size([5621, 492, 12])\n",
      "Valset:  \tx-torch.Size([784, 492, 12])  \ty-torch.Size([784, 492, 12])\n",
      "Testset:\tx-torch.Size([1590, 492, 12])\ty-torch.Size([1590, 492, 12])\n",
      "x- torch.Size([32, 492, 12])\n",
      "inp- torch.Size([32, 492, 12])\n",
      "sup- torch.Size([32, 492, 64])\n",
      "sup- torch.Size([492, 64, 32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (15744x64 and 492x492)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_54830/998663780.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_54830/4156769339.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, trainset_loader, valset_loader, optimizer, criterion, max_epochs, early_stop, verbose, gpu, plot)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_54830/4156769339.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, trainset_loader, optimizer, criterion, gpu)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mout_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mbatch_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_54830/3684482469.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x-\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrelation_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrelation_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch1.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_54830/3684482469.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, adj)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0msupport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msupport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sup-\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (15744x64 and 492x492)"
     ]
    }
   ],
   "source": [
    "data=read_data(\"../data/sz_taxi_202006/sz_taxi_202006_5min_recovered_flow_dlt.pkl\", transpose=True)\n",
    "train_loader, val_loader, test_loader=get_dataloaders(data, 12, 12)\n",
    "model=DontKnowWhat2EatNN(embed_vectors, 12, 12)\n",
    "criterion=torch.nn.MSELoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train(model, train_loader, val_loader, test_loader, criterion, gpu=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a8de3ed3a1560fc9f13af5266ca08823e90fcdd17bfae2dfaf8ec66fe041076c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('dz')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
