{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/cseadmin/dz/TrafficFlowModel/model'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from lib.metrics import evaluate\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "DATA_PATH=\"../data/sz_taxi_202006/\"\n",
    "SEQ_LEN=5\n",
    "EMBED_DIM=512\n",
    "NUM_ROADS=492\n",
    "\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_xy(traj_list, seq_len):\n",
    "    \"\"\"\n",
    "    Generate inputs and targets for traj next-hop prediction.\n",
    "    \n",
    "    Parameter\n",
    "    ---\n",
    "    traj_list: list of traj\n",
    "    ```\n",
    "    [\n",
    "      [246, 0, 70, 316, 246, 0, 70],\n",
    "      [265, 264, 261, 259, 255, 8, 60, 61, 111, 115, 79, 80, 81, 82, 164, 414],\n",
    "      ...\n",
    "    ]\n",
    "    ```\n",
    "    \n",
    "    Returns\n",
    "    ---\n",
    "    x: (num_samples, seq_len)\n",
    "    y: (num_samples,) 1-d vec for labels\n",
    "    \"\"\"\n",
    "    \n",
    "    x, y=[], []\n",
    "    for traj in traj_list:\n",
    "        for i in range(len(traj)-seq_len):\n",
    "            x.append(traj[i:i+seq_len])\n",
    "            y.append(traj[i+seq_len])\n",
    "            \n",
    "    return torch.LongTensor(x), torch.LongTensor(y)\n",
    "\n",
    "def get_dataloaders(traj_list, seq_len, train_size=0.7, val_size=0.1, batch_size=256):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ---\n",
    "    traj_list: list of traj\n",
    "    \"\"\"\n",
    "    np.random.shuffle(traj_list)\n",
    "    \n",
    "    split1=int(len(traj_list)*train_size)\n",
    "    split2=int(len(traj_list)*(train_size+val_size))\n",
    "    \n",
    "    train_data=traj_list[:split1]\n",
    "    val_data=traj_list[split1:split2]\n",
    "    test_data=traj_list[split2:]\n",
    "    \n",
    "    x_train, y_train=gen_xy(train_data, seq_len)\n",
    "    x_val, y_val=gen_xy(val_data, seq_len)\n",
    "    x_test, y_test=gen_xy(test_data, seq_len)\n",
    "    \n",
    "    print(f\"Trainset:\\tx-{x_train.size()}\\ty-{y_train.size()}\")\n",
    "    print(f\"Valset:  \\tx-{x_val.size()}  \\ty-{y_val.size()}\")\n",
    "    print(f\"Testset:\\tx-{x_test.size()}\\ty-{y_test.size()}\")\n",
    "    \n",
    "    trainset=torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    valset=torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    testset=torch.utils.data.TensorDataset(x_test, y_test)\n",
    "    \n",
    "    trainset_loader=torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    valset_loader=torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True)\n",
    "    testset_loader=torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return trainset_loader, valset_loader, testset_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def onehot_decode(label):\n",
    "    return torch.argmax(label, dim=1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def accuracy(predictions, targets):\n",
    "    \"\"\"\n",
    "    Computes the prediction accuracy, i.e., the average of correct predictions\n",
    "    of the network.\n",
    "    Args:\n",
    "        predictions: 2D float array of size [number_of_data_samples, n_classes]\n",
    "        labels: 2D int array of size [number_of_data_samples, n_classes] with one-hot encoding of ground-truth labels\n",
    "    Returns:\n",
    "        accuracy: scalar float, the accuracy of predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    pred_decode = onehot_decode(predictions)\n",
    "    true_decode = targets\n",
    "\n",
    "    assert (len(pred_decode) == len(true_decode))\n",
    "\n",
    "    acc = torch.mean((pred_decode == true_decode).float())\n",
    "\n",
    "    return float(acc)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, valset_loader, criterion, gpu=True):\n",
    "    model.eval()\n",
    "    batch_loss_list=[]\n",
    "    batch_acc_list=[]\n",
    "    for x_batch, y_batch in valset_loader:\n",
    "        if gpu and torch.cuda.is_available():\n",
    "            x_batch = x_batch.cuda()\n",
    "            y_batch = y_batch.cuda()\n",
    "\n",
    "        out_batch = model.forward(x_batch)\n",
    "        loss = criterion.forward(out_batch, y_batch)\n",
    "        batch_loss_list.append(loss.item())\n",
    "        \n",
    "        acc = accuracy(out_batch, y_batch)\n",
    "        batch_acc_list.append(acc)\n",
    "\n",
    "    # return sum(batch_loss_list)/len(batch_loss_list), sum(batch_acc_list)/len(batch_acc_list)\n",
    "    return np.mean(batch_loss_list), np.mean(batch_acc_list)\n",
    "\n",
    "def train_one_epoch(model, trainset_loader, optimizer, criterion, gpu=True):\n",
    "    model.train()\n",
    "    batch_loss_list=[]\n",
    "    batch_acc_list=[]\n",
    "    for x_batch, y_batch in trainset_loader:\n",
    "        if gpu and torch.cuda.is_available():\n",
    "            x_batch = x_batch.cuda()\n",
    "            y_batch = y_batch.cuda()\n",
    "\n",
    "        out_batch = model.forward(x_batch)\n",
    "        loss = criterion.forward(out_batch, y_batch)\n",
    "        batch_loss_list.append(loss.item())\n",
    "        \n",
    "        acc = accuracy(out_batch, y_batch)\n",
    "        batch_acc_list.append(acc)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # return sum(batch_loss_list)/len(batch_loss_list), sum(batch_acc_list)/len(batch_acc_list)\n",
    "    return np.mean(batch_loss_list), np.mean(batch_acc_list)\n",
    "\n",
    "def train(model, trainset_loader, valset_loader, optimizer, criterion, max_epochs=100, early_stop=10, verbose=1, gpu=True, plot=False, log=\"train.log\"):\n",
    "    if log:\n",
    "        log=open(log, \"a\")\n",
    "        log.seek(0)\n",
    "        log.truncate()\n",
    "    \n",
    "    wait=0\n",
    "    min_val_loss=np.inf\n",
    "    \n",
    "    train_loss_list=[]\n",
    "    train_acc_list=[]\n",
    "    val_loss_list=[]\n",
    "    val_acc_list=[]\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        train_loss, train_acc=train_one_epoch(model, trainset_loader, optimizer, criterion, gpu)\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_acc_list.append(train_acc)\n",
    "        \n",
    "        val_loss, val_acc=eval_model(model, valset_loader, criterion)\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_acc_list.append(val_acc)\n",
    "        \n",
    "        if (epoch+1)%verbose==0:\n",
    "            print(datetime.datetime.now(), \"Epoch\", epoch + 1,\n",
    "                \"\\tTrain Loss = %.5f\" % train_loss,\n",
    "                \"Train acc = %.5f \" % train_acc,\n",
    "                \"Eval Loss = %.5f\" % val_loss,\n",
    "                \"Eval acc = %.5f \" % val_acc)\n",
    "            \n",
    "            if log:\n",
    "                print(datetime.datetime.now(), \"Epoch\", epoch + 1,\n",
    "                \"\\tTrain Loss = %.5f\" % train_loss,\n",
    "                \"Train acc = %.5f \" % train_acc,\n",
    "                \"Eval Loss = %.5f\" % val_loss,\n",
    "                \"Eval acc = %.5f \" % val_acc,\n",
    "                file=log)\n",
    "                log.flush()\n",
    "        \n",
    "        if val_loss<min_val_loss:\n",
    "            wait=0\n",
    "            min_val_loss=val_loss\n",
    "            best_epoch=epoch\n",
    "        else:\n",
    "            wait+=1\n",
    "            if wait >= early_stop:\n",
    "                print(\"Early stopping at epoch: %d\" % epoch+1)\n",
    "                print(f\"Best at epoch {best_epoch+1}:\")\n",
    "                print(\"Train Loss = %.5f\" % train_loss_list[best_epoch], \"Train acc = %.5f \" % train_acc_list[best_epoch])\n",
    "                \n",
    "                if log:\n",
    "                    print(\"Early stopping at epoch: %d\" % epoch+1, file=log)\n",
    "                    print(f\"Best at epoch {best_epoch+1}:\", file=log)\n",
    "                    print(\"Train Loss = %.5f\" % train_loss_list[best_epoch], \"Train acc = %.5f \" % train_acc_list[best_epoch], file=log)\n",
    "                    log.flush()\n",
    "                break\n",
    "        \n",
    "    if plot:\n",
    "        plt.plot(range(0, epoch+1), train_loss_list, \"-\", label=\"Train Loss\")\n",
    "        plt.plot(range(0, epoch+1), val_loss_list, \"-\", label=\"Val Loss\")\n",
    "        plt.title(\"Epoch-Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    if log:\n",
    "        log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DontKnowWhat2EatNN(torch.nn.Module):\n",
    "    def __init__(self, embed_dim=512, hidden_dim=512, dropout=0.0):\n",
    "        super(DontKnowWhat2EatNN, self).__init__()\n",
    "        \n",
    "        self.embedding=torch.nn.Embedding(NUM_ROADS, embed_dim, padding_idx=-1)\n",
    "        self.rnn=torch.nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim, num_layers=1, batch_first=True, dropout=dropout)\n",
    "        self.fc=torch.nn.Linear(in_features=hidden_dim, out_features=NUM_ROADS)\n",
    "        self.softmax=torch.nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        out=self.embedding(x) # (batch_size, seq_len, embed_dim)\n",
    "        out, (_, _)=self.rnn(out) # (batch_size, seq_len, hidden_dim)\n",
    "        out=out[:, -1, :] # (batch_size, hidden_dim) get last step's output\n",
    "        out=self.fc(out) # (batch_size, num_roads)\n",
    "        out=self.softmax(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def get_embed_matrix(self):\n",
    "        return self.embedding.weight.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67576"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[318,\n",
       " 36,\n",
       " 39,\n",
       " 41,\n",
       " 43,\n",
       " 46,\n",
       " 50,\n",
       " 53,\n",
       " 56,\n",
       " 109,\n",
       " 116,\n",
       " 123,\n",
       " 127,\n",
       " 128,\n",
       " 130,\n",
       " 131,\n",
       " 159,\n",
       " 188,\n",
       " 189,\n",
       " 197]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=0.05\n",
    "traj_list_all=np.load(f\"../data/sz_taxi_202006/sz_taxi_202006_traj_list_bin_24_sampled_{p}_flatten_id.npy\", allow_pickle=True)\n",
    "\n",
    "len(traj_list_all)\n",
    "traj_list_all[6666]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset:\tx-torch.Size([656472, 5])\ty-torch.Size([656472])\n",
      "Valset:  \tx-torch.Size([94114, 5])  \ty-torch.Size([94114])\n",
      "Testset:\tx-torch.Size([189642, 5])\ty-torch.Size([189642])\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader=get_dataloaders(traj_list_all, SEQ_LEN, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-23 22:23:06.661806 Epoch 1 \tTrain Loss = 5.59580 Train acc = 0.62496  Eval Loss = 5.49917 Eval acc = 0.70340 \n",
      "2022-04-23 22:23:16.270657 Epoch 2 \tTrain Loss = 5.49145 Train acc = 0.71081  Eval Loss = 5.48946 Eval acc = 0.71274 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/cseadmin/dz/TrafficFlowModel/model/road_embed.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.16.27.18/home/cseadmin/dz/TrafficFlowModel/model/road_embed.ipynb#ch0000009vscode-remote?line=1'>2</a>\u001b[0m criterion\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.16.27.18/home/cseadmin/dz/TrafficFlowModel/model/road_embed.ipynb#ch0000009vscode-remote?line=2'>3</a>\u001b[0m optimizer\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.16.27.18/home/cseadmin/dz/TrafficFlowModel/model/road_embed.ipynb#ch0000009vscode-remote?line=3'>4</a>\u001b[0m train(model, train_loader, val_loader, optimizer, criterion, max_epochs\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, early_stop\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, plot\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, gpu\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, log\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain.log\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/home/cseadmin/dz/TrafficFlowModel/model/road_embed.ipynb Cell 5'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, trainset_loader, valset_loader, optimizer, criterion, max_epochs, early_stop, verbose, gpu, plot, log)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.16.27.18/home/cseadmin/dz/TrafficFlowModel/model/road_embed.ipynb#ch0000006vscode-remote?line=80'>81</a>\u001b[0m val_acc_list\u001b[39m=\u001b[39m[]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.16.27.18/home/cseadmin/dz/TrafficFlowModel/model/road_embed.ipynb#ch0000006vscode-remote?line=82'>83</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.16.27.18/home/cseadmin/dz/TrafficFlowModel/model/road_embed.ipynb#ch0000006vscode-remote?line=83'>84</a>\u001b[0m     train_loss, train_acc\u001b[39m=\u001b[39mtrain_one_epoch(model, trainset_loader, optimizer, criterion, gpu)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.16.27.18/home/cseadmin/dz/TrafficFlowModel/model/road_embed.ipynb#ch0000006vscode-remote?line=84'>85</a>\u001b[0m     train_loss_list\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.16.27.18/home/cseadmin/dz/TrafficFlowModel/model/road_embed.ipynb#ch0000006vscode-remote?line=85'>86</a>\u001b[0m     train_acc_list\u001b[39m.\u001b[39mappend(train_acc)\n",
      "\u001b[1;32m/home/cseadmin/dz/TrafficFlowModel/model/road_embed.ipynb Cell 5'\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, trainset_loader, optimizer, criterion, gpu)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.16.27.18/home/cseadmin/dz/TrafficFlowModel/model/road_embed.ipynb#ch0000006vscode-remote?line=61'>62</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.16.27.18/home/cseadmin/dz/TrafficFlowModel/model/road_embed.ipynb#ch0000006vscode-remote?line=62'>63</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.16.27.18/home/cseadmin/dz/TrafficFlowModel/model/road_embed.ipynb#ch0000006vscode-remote?line=63'>64</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.16.27.18/home/cseadmin/dz/TrafficFlowModel/model/road_embed.ipynb#ch0000006vscode-remote?line=65'>66</a>\u001b[0m \u001b[39m# return sum(batch_loss_list)/len(batch_loss_list), sum(batch_acc_list)/len(batch_acc_list)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.16.27.18/home/cseadmin/dz/TrafficFlowModel/model/road_embed.ipynb#ch0000006vscode-remote?line=66'>67</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(batch_loss_list), np\u001b[39m.\u001b[39mmean(batch_acc_list)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/optimizer.py?line=85'>86</a>\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/optimizer.py?line=86'>87</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/optimizer.py?line=87'>88</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/adam.py?line=137'>138</a>\u001b[0m             \u001b[39m# record the step after step update\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/adam.py?line=138'>139</a>\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/adam.py?line=140'>141</a>\u001b[0m     F\u001b[39m.\u001b[39;49madam(params_with_grad,\n\u001b[1;32m    <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/adam.py?line=141'>142</a>\u001b[0m            grads,\n\u001b[1;32m    <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/adam.py?line=142'>143</a>\u001b[0m            exp_avgs,\n\u001b[1;32m    <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/adam.py?line=143'>144</a>\u001b[0m            exp_avg_sqs,\n\u001b[1;32m    <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/adam.py?line=144'>145</a>\u001b[0m            max_exp_avg_sqs,\n\u001b[1;32m    <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/adam.py?line=145'>146</a>\u001b[0m            state_steps,\n\u001b[1;32m    <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/adam.py?line=146'>147</a>\u001b[0m            amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/adam.py?line=147'>148</a>\u001b[0m            beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/adam.py?line=148'>149</a>\u001b[0m            beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/adam.py?line=149'>150</a>\u001b[0m            lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/adam.py?line=150'>151</a>\u001b[0m            weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/adam.py?line=151'>152</a>\u001b[0m            eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/adam.py?line=152'>153</a>\u001b[0m            maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/adam.py?line=153'>154</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/_functional.py:105\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/_functional.py?line=102'>103</a>\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(bias_correction2))\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/_functional.py?line=103'>104</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/_functional.py?line=104'>105</a>\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(bias_correction2))\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/_functional.py?line=108'>109</a>\u001b[0m step_size \u001b[39m=\u001b[39m lr \u001b[39m/\u001b[39m bias_correction1\n\u001b[1;32m    <a href='file:///home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/torch/optim/_functional.py?line=109'>110</a>\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model=DontKnowWhat2EatNN(embed_dim=128, hidden_dim=128).cuda()\n",
    "criterion=torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train(model, train_loader, val_loader, optimizer, criterion, max_epochs=1000, early_stop=15, verbose=1, plot=False, gpu=True, log=\"train.log\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c06479978a9afb049dacef30f4b37fb04e7a2dfd65fe1755a86e45fec2f65ba"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('torch1.7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
